{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCN+Transformer Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data Augmentation Function\n",
    "def augment_data(X):\n",
    "    augmented_data = []\n",
    "    for sample in X:\n",
    "        noise = np.random.normal(0, 0.01, sample.shape)\n",
    "        augmented_data.append(sample + noise)\n",
    "    return np.array(augmented_data)\n",
    "\n",
    "# TCN Block Definition\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                               stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define the TCN block for each sensor\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, input_size, num_channels):\n",
    "        super(TCNBlock, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tcn(x)\n",
    "\n",
    "# Define the fusion and Transformer model\n",
    "class FusionTransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, tcn_channels, transformer_hidden_dim, num_layers, num_heads, num_classes=2):\n",
    "        super(FusionTransformerModel, self).__init__()\n",
    "        self.frontends = nn.ModuleList([TCNBlock(input_size=1, num_channels=tcn_channels) for _ in range(input_dim)])\n",
    "\n",
    "        tcn_output_dim = tcn_channels[-1]  # The output dimension of the last TCN layer\n",
    "        self.fc1 = nn.Linear(tcn_output_dim * input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, (tcn_output_dim * input_dim) // num_heads * num_heads)  # Ensure divisibility\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=(tcn_output_dim * input_dim) // num_heads * num_heads, nhead=num_heads, dim_feedforward=transformer_hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear((tcn_output_dim * input_dim) // num_heads * num_heads, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1).to(device)\n",
    "        \n",
    "        front_end_outputs = [front_end(x[:, i:i+1, :]) for i, front_end in enumerate(self.frontends)]\n",
    "        combined = torch.cat([output[:, :, -1] for output in front_end_outputs], dim=1)\n",
    "        combined = self.fc1(combined)\n",
    "        combined = self.relu(combined)\n",
    "        combined = self.fc2(combined)\n",
    "\n",
    "        combined = combined.unsqueeze(1)  # Add sequence dimension\n",
    "        combined = self.transformer_encoder(combined)\n",
    "        combined = self.dropout(combined)\n",
    "        combined = combined[:, -1, :]  # Take the last time step output from the Transformer\n",
    "\n",
    "        output = self.fc3(combined)\n",
    "        return output\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val, input_dim, tcn_channels, transformer_hidden_dim, num_layers, num_heads, num_classes=2, epochs=100, batch_size=32, learning_rate=0.001, patience=10, fold_index=0):\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = FusionTransformerModel(input_dim, tcn_channels, transformer_hidden_dim, num_layers, num_heads, num_classes).to(device)\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)  # Learning rate scheduler\n",
    "\n",
    "    writer = SummaryWriter(f'runs/fold_{fold_index}')  # TensorBoard writer\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item() * val_inputs.size(0)\n",
    "                _, val_predicted = torch.max(val_outputs, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', epoch_accuracy, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        scheduler.step(val_loss)  # Step the learning rate scheduler based on the validation loss\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Calculate number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    input_size = (1, input_dim, X_train.shape[2])  # Using batch size 1 for FLOPs calculation\n",
    "    flops = torchprofile.profile_macs(model, torch.randn(input_size).to(device))\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "\n",
    "    return model, epoch_loss\n",
    "\n",
    "def evaluate_model(models, X_test, y_test, batch_size=32):\n",
    "    # Aggregate predictions from all models\n",
    "    test_preds = []\n",
    "    test_losses = []\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        preds = []\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "            for data, labels in test_loader:\n",
    "                outputs = model(data)\n",
    "                preds.append(outputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                total += labels.size(0)\n",
    "\n",
    "        test_losses.append(running_loss / total)\n",
    "        test_preds.append(torch.cat(preds, dim=0))\n",
    "\n",
    "    # Average predictions\n",
    "    mean_preds = torch.mean(torch.stack(test_preds), dim=0)\n",
    "    _, predicted = torch.max(mean_preds, 1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "    precision = precision_score(y_test_tensor.cpu(), predicted.cpu(), average='weighted')\n",
    "    recall = recall_score(y_test_tensor.cpu(), predicted.cpu(), average='weighted')\n",
    "    f1 = f1_score(y_test_tensor.cpu(), predicted.cpu(), average='weighted')\n",
    "    mean_loss = np.mean(test_losses)\n",
    "    \n",
    "    return accuracy, mean_loss, precision, recall, f1\n",
    "\n",
    "# Perform Group K-Fold Evaluation\n",
    "\n",
    "NUM_MODELS = 5  # Number of models to train in the ensemble\n",
    "DNNS = []\n",
    "\n",
    "input_dim = X_res.shape[-1]  # Number of input channels (updated to include accelerometer data)\n",
    "tcn_channels = [25, 25, 25]  # TCN channels\n",
    "transformer_hidden_dim = 64\n",
    "num_layers = 2\n",
    "num_heads = 4  # Set the number of heads to a value that divides the embedding dimension\n",
    "num_classes = 2\n",
    "\n",
    "# Assuming I_TRAINS and I_TESTS are provided for Group K-Fold\n",
    "\n",
    "for fold_index, I_train in enumerate(I_TRAINS):\n",
    "    models = []\n",
    "    for _ in range(NUM_MODELS):\n",
    "        # Split training data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_res[I_train], y_simple[I_train], test_size=0.2, random_state=42)\n",
    "        \n",
    "        X_train_augmented = augment_data(X_train)\n",
    "        X_train_combined = np.concatenate((X_train, X_train_augmented))\n",
    "        y_train_combined = np.concatenate((y_train, y_train))  # Duplicate labels for augmented data\n",
    "\n",
    "        signal_length = X_train.shape[2]  # Use the actual signal length\n",
    "        model, epoch_loss = train_model(X_train_combined, y_train_combined, X_val, y_val, input_dim, tcn_channels, transformer_hidden_dim, num_layers, num_heads, num_classes, epochs=150, batch_size=32, learning_rate=0.0005, patience=10, fold_index=fold_index)  # Adjusted learning rate and added patience\n",
    "        models.append(model)\n",
    "        print(f\"Fold {fold_index + 1}, Model Loss: {epoch_loss:.4f}\")\n",
    "    DNNS.append(models)\n",
    "\n",
    "# Evaluate the ensemble of models for each fold\n",
    "accuracies = []\n",
    "mean_losses = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "for models, I_test in zip(DNNS, I_TESTS):\n",
    "    X_test, y_test = X_res[I_test], y_simple[I_test]\n",
    "\n",
    "    accuracy, mean_loss, precision, recall, f1 = evaluate_model(models, X_test, y_test)\n",
    "    accuracies.append(accuracy)\n",
    "    mean_losses.append(mean_loss)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate and print the mean and standard deviation of accuracies and other metrics\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_dev_accuracy = np.std(accuracies)\n",
    "mean_loss = np.mean(mean_losses)\n",
    "std_dev_loss = np.std(mean_losses)\n",
    "mean_precision = np.mean(precisions)\n",
    "std_dev_precision = np.std(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "std_dev_recall = np.std(recalls)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_dev_f1 = np.std(f1_scores)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%, (SD={std_dev_accuracy})\")\n",
    "print(f\"Mean Loss: {mean_loss:.4f}, (SD={std_dev_loss})\")\n",
    "print(f\"Mean Precision: {mean_precision:.4f}, (SD={std_dev_precision})\")\n",
    "print(f\"Mean Recall: {mean_recall:.4f}, (SD={std_dev_recall})\")\n",
    "print(f\"Mean F1 Score: {mean_f1:.4f}, (SD={std_dev_f1})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
